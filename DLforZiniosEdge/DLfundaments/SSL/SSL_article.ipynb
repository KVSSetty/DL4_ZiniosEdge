{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Supervised Learning in Machine Learning gets us closer to Human-like Autonoumos Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***self-supervised learning is the cake, supervised learning is the icing on the cake, reinforcement learning is the cherry on the cake*** - *Yann LeCun*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Self-Supervised Learning** is getting attention because it has the potential to solve a significant limitation of **supervised machine learning**, viz. requiring lots of external training samples or supervisory data consisting of inputs and corresponding outputs. Yann LeCun recently, in a **Science and Future Magazine** interview presented self-supervised learning as a significant challenge of AI for the next decade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Humans, for instance, can determine the semantic meaning of the word “orange” from context when it appears near “t-shirt,” “fridge,” “county,” or “mobile.” Similarly, in machine learning, the Word2Vec algorithm predicts the semantic context of a word based on surrounding words. The research behind self-supervised learning follows the same principle of automatically identifying, extracting and using supervisory signals ( ie targets or labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `What is Self-Supervised Learning?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-supervised learning is autonomous supervised learning. It is a representation learning approach that eliminates the pre-requisite requiring humans to label data. Self-supervised learning systems extract and use the naturally available relevant context and embedded metadata as supervisory signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cats continue to play an essential role in everything significant in machine learning. Self-supervised research [“Unsupervised Visual Representation Learning by Context Prediction”](https://arxiv.org/abs/1505.05192) predicts the positional location of one rectangular section of an image relative to another by using spatial context as a supervisory signal for training a rich visual representation. For instance, the right ear of a cat would be in the top-right position relative to the eyes of a cat. This approach allows learning about cats, dogs, or buses without prior explicit semantic labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![catimage](image01.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-supervised learning, fortunately, is not limited to learning from visual cues or associated meta-data in cat images or videos and has use cases beyond computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Self-supervised vs. supervised learning`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-supervised Learning is supervised Learning because its goal is to learn a function from pairs of inputs and labeled outputs. Explicit use of labeled input-outputs pairs in self-supervised learning is not needed. Instead, correlations, embedded metadata, or domain knowledge available within the input is implicitly and autonomously extracted from the data and used as supervisory signals. Like supervised learning, self-supervised learning has use cases in regression and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Self-supervised vs. unsupervised learning`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-supervised learning is like unsupervised Learning because the system learns without using explicitly-provided labels. It is different from unsupervised learning because we are not learning the inherent structure of data. Self-supervised learning, unlike unsupervised learning, is not centered around clustering and grouping, dimensionality reduction, recommendation engines, density estimation, or anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Self-Supervised vs. semi-supervised learning`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combination of labeled and unlabeled data is used to train semi-supervised learning algorithms, where smaller amounts of labeled data in conjunction with large amounts of unlabeled data can speed up learning tasks. Self-supervised learning is different as systems learn entirely without using explicitly-provided labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Self-Supervised vs Reinforcement Learning`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dog trainer can reward a dog for positive behavior and punish for negative behavior. Over time the dog figures out and learns actions it took to get a reward. Similarly, in Reinforcement learning, a navigating robot learns how to navigate a track when rewarded for staying on track and punished when it collides with something in the environment. In both cases, this reward and punishment feedback reinforces which actions to perform and which to avoid. Reinforcement Learning works well in the presence of a feedback system for rewards. It also requires a comprehensive set of training data and may be impractical when considering the cost, time, and the number of iterations required before succeeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the absence of rewards-based feedback system, a dog or a navigating robot may learn on its own by curiously exploring the environment. Researchers from BAIR ( Berkeley Artificial Intelligence Research: http://bair.berkeley.edu) created an **“Intrinsic Curiosity Model,”** a self-supervised reinforcement learning system that can work even in the absence of explicit feedback. It uses curiosity as a natural reward signal to enable the agent to explore its environment and learn skills for use later in its life. See reseach at: [Curiosity-driven Exploration by Self-supervised Prediction](https://arxiv.org/abs/1705.05363) and the accompanying [writeup.](https://pathak22.github.io/noreward-rl/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using self-supervised learning machines can predict through natural evolution and consequences of its actions, similar to how newborns can learn incredible amounts of information in their first weeks/months of life by observing and being curious. Self-supervised learning has the potential to scale learning to levels required by new use cases including but not limited to use cases in medicine, autonomous driving, robotics, language understanding, and image recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Why is self-supervised learning relevant?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-supervised learning is essential for many reasons but mainly because of shortcomings in both approach and scalability of supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning is an arduous process, requiring collecting massive amounts of data, cleaning it up, manually labeling it, training and perfecting a model purpose-built for the classification or regression use case you are solving for, and then using it to predict labels for unknown data. For instance, with images, we collect a large image data set, label the objects in images manually, learn the network and then use it for one specific use case. This way is very different from the approach of learning in humans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human learning is trial-based, perpetual, multi-sourced, and simultaneous for multiple tasks. We learn mostly in an unsupervised manner, using experiments and curiosity. We also learn in a supervised manner but we can learn from much fewer samples, and we generalize exceptionally well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For supervised learning, we have spent years collecting and professionally annotating tens of millions of labeled bounding boxes or polygons and image level annotations, but these datasets [Open Images](https://storage.googleapis.com/openimages/web/index.html), [PASCAL Visual Object Classes](http://host.robots.ox.ac.uk/pascal/VOC/index.html), [Image Net](http://www.image-net.org/), and Microsoft [COCO](http://www.image-net.org/) collectively pale in comparison to billions of images generated on a daily basis on social media, or millions of videos requiring object detection or depth perception in autonomous driving. Similar scalability arguments exist for common sense knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Self-supervised learning brings us closer to human-like autonomous learning.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-Supervised Learning has found success in [estimating relative scene depths without human supervision](https://arxiv.org/abs/1712.04850), by using motion segmentation techniques to determine relative depth from geometric constraints between the scene’s motion field and camera motion.\n",
    "\n",
    "In medicine, it has found use cases [robotic surgery](https://arxiv.org/abs/1705.08260) and in [dense depth estimation in monocular endoscopy](https://arxiv.org/abs/1806.09521).\n",
    "\n",
    "In autonomous driving, self-supervised learning is useful for [estimating the roughness of the terrain](https://arxiv.org/abs/1206.6872) when off-roading. It has a use case in [depth completion using LiDAR and monocular cameras.](https://arxiv.org/abs/1807.00275]\n",
    "\n",
    "To explore latest self-supervised learning use cases, see this search [link.](https://arxiv.org/search/?searchtype=title&query=Self-Supervised&size=200&order=-submitted_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Some More useful defnitions , notable points and links about SSL`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-supervised learning (or self-supervision) is a relatively recent learning technique (in machine learning) where the training data is autonomously (or automatically) labelled. It is still supervised learning, but the datasets do not need to be manually labelled by a human, but they can be labelled by finding and exploiting the relations (or correlations) between different input signals (that is, input coming from different sensor modalities)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A natural advantage and consequence of self-supervised learning is that it can more easily (with respect to e.g. supervised learning) be performed in an online fashion (given that data can be gathered and labelled without human intervention), where models can be updated or trained completely from scratch. Therefore, self-supervised learning should also be well suited for changing environments, data and, in general, situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that self-supervised learning is defined slightly differently depending on the context or area, which can, for example, be robotics, reinforcement learning or representation (or feature) learning. More precisely, the definition given above is used in robotics. See, for example, also this paper [Multi-task Self-Supervised Visual Learning](https://arxiv.org/pdf/1708.07860.pdf). For a slightly different definition of self-supervised learning, see, for example, the paper [Digging Into Self-Supervised Monocular Depth Estimation.](https://arxiv.org/pdf/1806.01260.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a curated list of links to papers where this learning approach is used at the following URL: https://github.com/jason718/awesome-self-supervised-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also [here](https://sites.google.com/view/self-supervised-icml2019#targetText=Overview,%2D%2D%20have%20become%20a%20bottleneck.) is yet another useful link on SSL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice overview and interesting results can be found in [this recent paper.](https://arxiv.org/pdf/1901.09005.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-supervised learning is when you use some parts of the samples as labels for a task that requires a good degree of comprehension to be solved. I'll emphasize these two key points, before giving an example:\n",
    "\n",
    "1. **Labels are extracted from the sample**, so they can be generated automatically, with some very simple algorithm.\n",
    "2. **The task requires understanding.**This means that, in order to predict the output, the model has to extract some good patterns from the data, generating on the process a good representation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very common case for semi-supervised learning takes place in natural language processing, when you need to solve a task but have few labeled data. In such cases, you need to learn a good representation or language model, so you take sentences and give your network self-supervision tasks like these:\n",
    "\n",
    "- Ask the network to predict the next word in a sentence (which you know because you took it away).\n",
    "\n",
    "- Mask a word and ask the network to predict which word goes there (which you know because you had to mask it).\n",
    "\n",
    "- Change the word for a random one (that probably doesn't make sense) and ask the network which word is wrong.\n",
    "\n",
    "As you can see, these tasks are fairly simple to formulate and the labels are part of the same sample, but they require a certain understanding of the context to be solved.\n",
    "\n",
    "And it's always like this: alter your data in some way, generating the label in the process, and ask the model something related to that transformation. If the task requires enough understanding of the data, you'll have success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hope this article along with the provided links clears lot of ground on the ***Self-Supervised Learning (SSL)*** that  has become an exciting direction in AI community. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
